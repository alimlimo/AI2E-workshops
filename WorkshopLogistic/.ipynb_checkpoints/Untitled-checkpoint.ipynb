{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/cover.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI2E - [Workshop 4] - [Introduction to ML Part 2 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understand and perform logistic regression on datasets with different number of features.  \n",
    "  \n",
    "<h3>Content</h3> \n",
    "<ol>\n",
    "    <li>Logistic Regression from Scratch.</li>  \n",
    "    <li>Logistic Regression with scikit-learn.</li>    \n",
    "    <li>Conclusion</li>\n",
    "</ol>    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1. Logistic Regression From Scratch </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('csv/1.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isna().sum()) #if there is missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.admit.value_counts()) #distributions of admit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df.pop('admit').values \n",
    "df.drop('id',axis=1,inplace=True) #remove the id column\n",
    "df = (df - df.mean()) / df.std() #normalize the data\n",
    "df['const'] = 1 #add a column that has only the value 1 for the regression\n",
    "nb_variables = len(df.columns) #count the number of features\n",
    "var_names = df.columns\n",
    "df = df.values #convert to numpy array\n",
    "x_train,x_test,y_train,y_test = train_test_split(df,target,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(x): \n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def model(estimators,df): #return the result of the logistic regression applied on df\n",
    "    return sigmoid(np.dot(df,estimators))\n",
    "\n",
    "def gradient(estimators,df,target): #compute the gradient of the log loss accoring to the estimators\n",
    "    return 1/len(df)*np.dot(df.transpose(),model(estimators,df) - target) \n",
    "\n",
    "def loss(estimators,df,target): #compute the log loss\n",
    "    return -np.log(model(estimators,df[target==1])).sum() - np.log(1-model(estimators,df[target==0])).sum()\n",
    "\n",
    "def train(df,target,epochs,lambd): #optimize the loss function\n",
    "    estimators = np.random.normal(0,1,nb_variables)\n",
    "    for epoch in range(epochs):\n",
    "        LOSS.append(loss(estimators,df,target))\n",
    "        gr = gradient(estimators,df,target)\n",
    "        estimators -= lambd * gr \n",
    "    return estimators    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 500\n",
    "LOSS  = []\n",
    "lambd = .01\n",
    "estimators = train(x_train,y_train,EPOCHS,lambd) \n",
    "plt.plot(range(len(LOSS)),LOSS)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(estimators,x_test,y_test):\n",
    "    predictions = model(estimators,x_test)\n",
    "    convert = np.vectorize(lambda x: 1 if x>.5 else 0)\n",
    "    predictions = convert(predictions)\n",
    "    print('the accuracy on test is ',1- ((predictions-y_test)**2).mean())\n",
    "accuracy(estimators,x_test,y_test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(var_names,estimators) #show the estimators\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. Logistic Regression with scikit-learn </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(penalty='l2',max_iter=100)\n",
    "lr.fit(x_train,y_train)\n",
    "print('accuracy on test is ',lr.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.bar(var_names,lr.coef_[0]+lr.intercept_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3. Conclusion </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear and Logistic Regression are the hello world of machine learning, those models are simple but sometimes not complexe enough to approximate complexe relationship, in the next lesson you will learn more powerful models like boosting trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
